[PE1] [Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach](./Personalization/PE1-personalized-federated-learning-with-theoretical-guarantees-a-model-agnostic-meta-learning-approach-Paper.pdf) 
NIPS 20

It studies a personalized variant of the federated learning in which the goal is to find an initial shared model that current or new users can easily adapt to their local dataset by performing one or a few steps of gradient descent with respect to their own data. This problem can be studied within the Model-Agnostic Meta-Learning (MAML) framework. Inspired by this connection, they study a personalized variant of the well-known Federated Averaging algorithm and evaluate its performance in terms of gradient norm for non-convex loss functions. Further, it characterize how this performance is affected by the closeness of underlying distributions of user data, measured in terms of distribution distances such as Total Variation and 1-Wasserstein metric.

[PE2] [Personalized Federated Learning with Moreau Envelopes](./Personalization/PE2-personalized-federated-learning-with-moreau-envelopes-Paper.pdf) NIPS 20

[PE3] [Adaptive Gradient-Based Meta-Learning Methods](./Personalization/PE3-adaptive-gradient-based-meta-learning-methods-Paper.pdf) NIPS 19